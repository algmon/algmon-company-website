---
sidebar_position: 3
---

# 3.Simple IR Example

The main feature of the embeddings is that the cosine similarity between two embeddings captures the semantic relatedness of the corresponding original passages. This allows us to use the embeddings to do semantic retrieval / search.

Suppose the user sends a "query" (e.g., a question or a comment) to the chatbot:

```python "
query = "算法妈妈是一家怎样的公司？"
```

To find out the document that is most similar to the query among the existing data, we can first embed/vectorize the query:

```python "
# Get the embedding of the query
query_embedding = get_embeddings(query, model="voyage-01")
```

Nearest neighbor Search: We can find a few closest embeddings in the documents embeddings based on the cosine similarity, and retrieve the corresponding document using the nearest_neighbors function.

```python "
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def k_nearest_neighbors(query_embedding, documents_embeddings, k=5):
    query_embedding = np.array(query_embedding) # convert to numpy array
    documents_embeddings = np.array(documents_embeddings) # convert to numpy array

    # Reshape the query vector embedding to a matrix of shape (1, n) to make it compatible with cosine_similarity
    query_embedding = query_embedding.reshape(1, -1)

    # Calculate the similarity for each item in data
    cosine_sim = cosine_similarity(query_embedding, documents_embeddings)

    # Sort the data by similarity in descending order and take the top k items
    sorted_indices = np.argsort(cosine_sim[0])[::-1]

    # Take the top k related embeddings
    top_k_related_indices = sorted_indices[:k]
    top_k_related_embeddings = documents_embeddings[sorted_indices[:k]]
    top_k_related_embeddings = [list(row[:]) for row in top_k_related_embeddings] # convert to list

    return top_k_related_embeddings, top_k_related_indices
```

```python "
# Use the nearest neighbor algorithm to find the document with the highest similarity
retrieved_embd, retrieved_embd_index = k_nearest_neighbors(query_embedding, documents_embeddings, k=1)
retrieved_doc = [documents[index] for index in retrieved_embd_index]

print(retrieved_doc)
```

Output:
```python "
"算法妈妈是一家人工智能公司，使用AI底座同时赋能时尚行业和教培行业。"
```

k-nearest neighbors Search (k-NN): It is often useful to retrieve not only the closest document but also the most k closest documents. The k_nearest_neighbors algorithm enables us to achieve this. It is important to note that nearest_neighbors is special case of k_nearest_neighbors when k=1.

```python "
# Use the k-nearest neighbor algorithm to identify the top-k documents with the highest similarity
retrieved_embds, retrieved_embd_indices = k_nearest_neighbors(query_embedding, documents_embeddings, k=3)
retrieved_docs = [documents[index] for index in retrieved_embd_indices]
```

## Vector Database

Vector Database is a specialized database or data platform designed to cater to the unique needs of applications and industries that rely heavily on vector-based data. This database is engineered to efficiently store, manage, and retrieve vector data, which can include a wide range of information such as spatial data, molecular sequences, time-series data, and more. VectorDB offers a robust set of tools and features tailored to support the indexing, querying, and analysis of vector data, making it a valuable resource for researchers, data scientists, and businesses working with complex datasets that exhibit vector-like characteristics.