---
sidebar_position: 0
---

# 0.大模型性能评估

我们在进行垂类大模型基准测试（即Benchmark Evaluation）时通常使用如下任务：

1. 基于知识的问答(Knowledge-based QA)
   1. MMLU测试
   2. MMMU测试
2. 通用推理(General-purpose Reasoning)
   1. BIG-Bench Hard
3. 数学能力
   1. GSM8k（小学数学基准测试）
   2. SVAMP（通过改变单词顺序生成问题来检查鲁棒性推理能力）
   3. ASDIV（具有不同的语言模式和问题类型）
   4. MAWPS（包含算术和代数词问题）
4. 代码生成
   1. HumanEval（测试模型对 Python 标准库中有限的一组函数的基本代码理解能力）
   2. ODEX（测试模型使用整个 Python 生态系统中更广泛的库集的能力）
5. 机器翻译
   1. FLORES-200
6. Web智能体
   1. 如WebArena