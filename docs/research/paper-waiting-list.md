---
sidebar_position: 99
---

# Paper Waiting List

## Queue

1.  Accessing the impact of promption, persona, and chain of thought methods on chatgpt's arithmetic capabilities.
    1.  我们系统测试了3种规定方法的效率，他们分别是策略提示，角色实施及思维链方法，实验表明并无显著提升ChatGPT的计算能力。
2.  AURORA: Activating Chinese Chat Capability for Mistral-8x7b Sparse Mixtur of Experts through Instruction Tuning.
    1.  为了评估Aurora的性能，我们使用了三个广泛认可的基准测试，证明执行指令微调的有效性。
3.  Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey
    1.  重点关注了基于 Transformer 的 LLM 模型体系结构在从预训练到推理的所有阶段中优化长上下文能力的进展。
4.  [大语言模型的数学之路](https://mp.weixin.qq.com/s/BXQVY7rjlwjAkSxWg_dtcQ?poc_token=HG6UlmWjJZrOzOoYBgzcvr4U8cvjzDAq4eRf8z7N)
6.  [LLM Augmented LLMs: Expanding Capabilities Through Composition](https://arxiv.org/pdf/2401.02412.pdf)
7.  [看见这张图没有，你就照着画：谷歌图像生成AI掌握多模态指令](https://mp.weixin.qq.com/s/u8qjCihAFZVVnuP82b-MSQ)
9.  LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning