---
sidebar_position: 5
---

# 平台能力
## Text Generation Models
Text generation models are now capable of JSON mode and Reproducible outputs. We also launched the Assistants API to enable you to build agent-like experiences on top of our text-generation models. GPT-4 Turbo is available in preview by specifying gpt-4-1106-preview as the model name.

OpenAI's text generation models (often called generative pre-trained transformers or large language models) have been trained to understand natural language, code, and images. The models provide text outputs in response to their inputs. The inputs to these models are also referred to as "prompts". Designing a prompt is essentially how you “program” a large language model model, usually by providing instructions or some examples of how to successfully complete a task.

Using OpenAI's text generation models, you can build applications to:

* Draft documents
* Write computer code
* Answer questions about a knowledge base
* Analyze texts
* Give software a natural language interface
* Tutor in a range of subjects
* Translate languages
* Simulate characters for games

With the release of gpt-4-vision-preview, you can now build systems that also process and understand images.

* Explore GPT-4 with image inputs
* GPT-4 Turbo

To use one of these models via the OpenAI API, you’ll send a request containing the inputs and your API key, and receive a response containing the model’s output. Our latest models, gpt-4 and gpt-3.5-turbo, are accessed through the chat completions API endpoint.

| | MODEL FAMILIES | API ENDPOINT |
| ----- | ----- | ----- |
| Newer models (2023–) | gpt-4 (and gpt-4 turbo), gpt-3.5-turbo | https://api.openai.com/v1/chat/completions |
| Updated base models (2023) | babbage-002, davinci-002 | https://api.openai.com/v1/completions |
| Legacy models (2020–2022) | text-davinci-003, text-davinci-002, davinci, curie, babbage, ada | https://api.openai.com/v1/completions |

You can experiment with various models in the chat playground. If you’re not sure which model to use, then use gpt-3.5-turbo or gpt-4.

### Chat Completions API

Chat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it’s just as useful for single-turn tasks without any conversation.

An example Chat Completions API call looks like the following:

```python "
from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Who won the world series in 2020?"},
    {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
    {"role": "user", "content": "Where was it played?"}
  ]
)
```

To learn more, you can view the full API reference documentation for the Chat API.

The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either "system", "user", or "assistant") and content. Conversations can be as short as one message or many back and forth turns.

Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages.

The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as "You are a helpful assistant."

The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.

Including conversation history is important when user instructions refer to prior messages. In the example above, the user’s final question of "Where was it played?" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model’s token limit, it will need to be shortened in some way.

* To mimic the effect seen in ChatGPT where the text is returned iteratively, set the stream parameter to true.

#### Chat Completions Response Format

* An example Chat Completions API response looks as follows:

```json "
{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "The 2020 World Series was played in Texas at Globe Life Field in Arlington.",
        "role": "assistant"
      }
    }
  ],
  "created": 1677664795,
  "id": "chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW",
  "model": "gpt-3.5-turbo-0613",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 17,
    "prompt_tokens": 57,
    "total_tokens": 74
  }
}
```

The assistant’s reply can be extracted with:

```python "
response['choices'][0]['message']['content']
```

Every response will include a finish_reason. The possible values for finish_reason are:

* stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter
* length: Incomplete model output due to max_tokens parameter or token limit
* function_call: The model decided to call a function
* content_filter: Omitted content due to a flag from our content filters
* null: API response still in progress or incomplete

Depending on input parameters, the model response may include different information.

### JSON mode

### Reproducible Outputs

### Managing Tokens

### FAQ

## Function Calling
## Embeddings
## Fine-Tuning
## Image Generation
## Vision
## Text-to-Speech
## Speech-to-Text
## Moderation